{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, random\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd \n",
    "import torch.nn.functional as F\n",
    "\n",
    "import copy\n",
    "import os\n",
    "import ipywidgets as widgets\n",
    "import tqdm\n",
    "# import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = [12, 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Use Cuda</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "Variable = lambda *args, **kwargs: autograd.Variable(*args, **kwargs).cuda() if USE_CUDA else autograd.Variable(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Replay Buffer</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        state      = np.expand_dims(state, 0)\n",
    "        next_state = np.expand_dims(next_state, 0)\n",
    "            \n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        state, action, reward, next_state, done = zip(*random.sample(self.buffer, batch_size))\n",
    "        return np.concatenate(state), action, reward, np.concatenate(next_state), done\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Smoothing</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_average(data, *, window_size):\n",
    "    \"\"\"Smoothen the 1-d data array using a rollin average.\n",
    "\n",
    "    Args:\n",
    "        data: 1-d numpy.array\n",
    "        window_size: size of the smoothing window\n",
    "\n",
    "    Returns:\n",
    "        smooth_data: a 1-d numpy.array with the same size as data\n",
    "    \"\"\"\n",
    "#     assert data.ndim == 1\n",
    "    kernel = np.ones(window_size)\n",
    "    smooth_data = np.convolve(data, kernel) / np.convolve(\n",
    "        np.ones_like(data), kernel\n",
    "    )\n",
    "    return smooth_data[: -window_size + 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Epsilon greedy exploration</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExponentialSchedule:\n",
    "    def __init__(self, value_from, value_to, num_steps):\n",
    "        \"\"\"Exponential schedule from `value_from` to `value_to` in `num_steps` steps.\n",
    "\n",
    "        $value(t) = a \\exp (b t)$\n",
    "\n",
    "        :param value_from: initial value\n",
    "        :param value_to: final value\n",
    "        :param num_steps: number of steps for the exponential schedule\n",
    "        \"\"\"\n",
    "        self.value_from = value_from\n",
    "        self.value_to = value_to\n",
    "        self.num_steps = num_steps\n",
    "\n",
    "        # YOUR CODE HERE:  determine the `a` and `b` parameters such that the schedule is correct\n",
    "        self.a = self.value_from\n",
    "        self.b = np.log(self.a/self.value_to)/ (self.num_steps-1)\n",
    "\n",
    "    def value(self, step) -> float:\n",
    "        \"\"\"Return exponentially interpolated value between `value_from` and `value_to`interpolated value between.\n",
    "\n",
    "        returns {\n",
    "            `value_from`, if step == 0 or less\n",
    "            `value_to`, if step == num_steps - 1 or more\n",
    "            the exponential interpolation between `value_from` and `value_to`, if 0 <= steps < num_steps\n",
    "        }\n",
    "        \n",
    "        :param step:  The step at which to compute the interpolation.\n",
    "        :rtype: float.  The interpolated value.\n",
    "        \"\"\"\n",
    "        \n",
    "        # YOUR CODE HERE:  implement the schedule rule as described in the docstring,\n",
    "        # using attributes `self.a` and `self.b`.\n",
    "        #value = ...\n",
    "        \n",
    "        if step <= 0:\n",
    "            value = self.value_from\n",
    "            return value\n",
    "        \n",
    "        if step >= self.num_steps - 1:\n",
    "            value = self.value_to\n",
    "            return value\n",
    "        \n",
    "        value = self.a/np.exp(self.b*step)\n",
    "        return value\n",
    "\n",
    "\n",
    "# test code, do not edit\n",
    "\n",
    "def _test_schedule(schedule, step, value, ndigits=5):\n",
    "    \"\"\"Tests that the schedule returns the correct value.\"\"\"\n",
    "    v = schedule.value(step)\n",
    "    if not round(v, ndigits) == round(value, ndigits):\n",
    "        raise Exception(\n",
    "            f'For step {step}, the scheduler returned {v} instead of {value}'\n",
    "        )\n",
    "\n",
    "\n",
    "_schedule = ExponentialSchedule(0.1, 0.2, 3)\n",
    "_test_schedule(_schedule, -1, 0.1)\n",
    "_test_schedule(_schedule, 0, 0.1)\n",
    "_test_schedule(_schedule, 1, 0.141421356237309515)\n",
    "_test_schedule(_schedule, 2, 0.2)\n",
    "_test_schedule(_schedule, 3, 0.2)\n",
    "del _schedule\n",
    "\n",
    "_schedule = ExponentialSchedule(0.5, 0.1, 5)\n",
    "_test_schedule(_schedule, -1, 0.5)\n",
    "_test_schedule(_schedule, 0, 0.5)\n",
    "_test_schedule(_schedule, 1, 0.33437015248821106)\n",
    "_test_schedule(_schedule, 2, 0.22360679774997905)\n",
    "_test_schedule(_schedule, 3, 0.14953487812212207)\n",
    "_test_schedule(_schedule, 4, 0.1)\n",
    "_test_schedule(_schedule, 5, 0.1)\n",
    "del _schedule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Computing Temporal Difference Loss</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_td_loss(batch_size):\n",
    "    state, action, reward, next_state, done = replay_buffer.sample(batch_size)\n",
    "\n",
    "    state      = Variable(torch.FloatTensor(np.float32(state)))\n",
    "    next_state = Variable(torch.FloatTensor(np.float32(next_state)))\n",
    "    action     = Variable(torch.LongTensor(action))\n",
    "    reward     = Variable(torch.FloatTensor(reward))\n",
    "    done       = Variable(torch.FloatTensor(done))\n",
    "\n",
    "    q_values      = current_model(state)\n",
    "    next_q_values = target_model(next_state)\n",
    "\n",
    "    q_value          = q_values.gather(1, action.unsqueeze(1)).squeeze(1)\n",
    "    next_q_value     = next_q_values.max(1)[0]\n",
    "    expected_q_value = reward + gamma * next_q_value * (1 - done)\n",
    "    \n",
    "    loss = (q_value - expected_q_value.detach()).pow(2).mean()\n",
    "        \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "#     return loss\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(frame_idx, returns, lengths, losses):\n",
    "    clear_output(True)\n",
    "    # YOUR PLOTS HERE\n",
    "    epochs = np.arange(len(returns))\n",
    "    # smooth data\n",
    "    smoothed_returns = rolling_average(returns, window_size = 100)\n",
    "    smoothed_lengths = rolling_average(lengths, window_size = 100)\n",
    "    smoothed_losses = rolling_average(losses, window_size = 100)\n",
    "\n",
    "    # plot the returns\n",
    "    fig = plt.figure(figsize=(14, 10))\n",
    "    ax1 = fig.add_subplot(311)\n",
    "    line_1, = ax1.plot(epochs, returns, 'salmon', linewidth=2.0)\n",
    "    line_2, = ax1.plot(epochs, smoothed_returns, 'r', linewidth=2.0)\n",
    "    plt.xlabel('Number of episodes')\n",
    "    plt.ylabel('Averaged returns')\n",
    "    plt.title('Returns')\n",
    "    plt.legend((line_1, line_2), ('raw', 'smoothed'))\n",
    "\n",
    "    ax2 = fig.add_subplot(312)\n",
    "    line_1, = ax2.plot(epochs, lengths, 'lightblue', linewidth=2.0)\n",
    "    line_2, = ax2.plot(epochs, smoothed_lengths, 'b', linewidth=2.0)\n",
    "    plt.xlabel('Number of episodes')\n",
    "    plt.ylabel('Averaged lengths')\n",
    "    plt.title('Lengths')\n",
    "    plt.legend((line_1, line_2), ('raw', 'smoothed'))\n",
    "    ax3 = fig.add_subplot(313)\n",
    "    epochs = np.arange(len(losses))\n",
    "    line_1, = ax3.plot(epochs, losses, 'lightgreen', linewidth=2.0)\n",
    "    line_2, = ax3.plot(epochs, smoothed_losses, 'g', linewidth=2.0)\n",
    "    plt.xlabel('Number of time step')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss')\n",
    "    plt.legend((line_1, line_2), ('raw', 'smoothed'))\n",
    "    plt.show()\n",
    "    \n",
    "#     plt.figure(figsize=(100,25))\n",
    "#     plt.subplot(131)\n",
    "#     plt.title('frame %s. reward: %s' % (frame_idx, np.mean(rewards[-10:])))\n",
    "#     plt.plot(returns)\n",
    "#     plt.subplot(132)\n",
    "#     plt.title('loss')\n",
    "#     plt.plot(losses)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Atari Environment</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from wrappers import make_atari, wrap_deepmind, wrap_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_id = \"BreakoutNoFrameskip-v4\"\n",
    "#env_id = \"PongNoFrameskip-v4\"\n",
    "env    = make_atari(env_id)\n",
    "env    = wrap_deepmind(env)\n",
    "env    = wrap_pytorch(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuelingCnnDQN(nn.Module):\n",
    "    def __init__(self, input_shape, num_outputs):\n",
    "        super(DuelingCnnDQN, self).__init__()\n",
    "        \n",
    "        \n",
    "        self.input_shape = input_shape\n",
    "        self.num_actions = num_outputs\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.advantage = nn.Sequential(\n",
    "            nn.Linear(self.feature_size(), 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, num_outputs)\n",
    "        )\n",
    "        \n",
    "        self.value = nn.Sequential(\n",
    "            nn.Linear(self.feature_size(), 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        advantage = self.advantage(x)\n",
    "        value     = self.value(x)\n",
    "        return value + advantage  - advantage.mean()\n",
    "    \n",
    "    def feature_size(self):\n",
    "        return self.features(autograd.Variable(torch.zeros(1, *self.input_shape))).view(1, -1).size(1)\n",
    "    \n",
    "    def act(self, state, epsilon):\n",
    "        if random.random() > epsilon:\n",
    "#             state   = Variable(torch.FloatTensor(np.float32(state)).unsqueeze(0), volatile=True)\n",
    "            with torch.no_grad():\n",
    "                state = Variable(torch.FloatTensor(np.float32(state)).unsqueeze(0)) \n",
    "            q_value = self.forward(state)\n",
    "            action  = q_value.max(1)[1].data[0]\n",
    "        else:\n",
    "            action = random.randrange(env.action_space.n)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_model = DuelingCnnDQN(env.observation_space.shape, env.action_space.n)\n",
    "target_model  = DuelingCnnDQN(env.observation_space.shape, env.action_space.n)\n",
    "\n",
    "if USE_CUDA:\n",
    "    current_model = current_model.cuda()\n",
    "    target_model  = target_model.cuda()\n",
    "    \n",
    "optimizer = optim.Adam(current_model.parameters(), lr=0.0001)\n",
    "\n",
    "replay_initial = 10_000\n",
    "replay_buffer = ReplayBuffer(100_000)\n",
    "# replay_initial = 50\n",
    "# replay_buffer = ReplayBuffer(100)\n",
    "\n",
    "update_target(current_model, target_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_frames = 2_000_000\n",
    "# num_frames = 1000\n",
    "batch_size = 32\n",
    "gamma      = 0.99\n",
    "\n",
    "losses = []\n",
    "returns = []\n",
    "lengths = []\n",
    "episode_reward = []\n",
    "t_episode = 0\n",
    "i_episode = 0\n",
    "exploration = ExponentialSchedule(1.0, 0.05, 1_000_000)\n",
    "\n",
    "state = env.reset()\n",
    "pbar = tqdm.tnrange(num_frames, ncols='100%')\n",
    "# for frame_idx in range(1, num_frames + 1):\n",
    "for frame_idx in pbar:\n",
    "#     epsilon = epsilon_by_frame(frame_idx)\n",
    "    epsilon = exploration.value(frame_idx)\n",
    "    action = current_model.act(state, epsilon)\n",
    "    \n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    replay_buffer.push(state, action, reward, next_state, done)\n",
    "    \n",
    "    state = next_state\n",
    "    episode_reward.append(reward)\n",
    "    \n",
    "    if done:\n",
    "        state = env.reset()\n",
    "        G = 0\n",
    "        for i in reversed(episode_reward):\n",
    "            G = i + G * gamma\n",
    "        returns.append(G)\n",
    "\n",
    "        episode_reward = []\n",
    "        pbar.set_description(\n",
    "                f'Episode: {i_episode} | Steps: {t_episode + 1} | Return: {G:5.2f} | Epsilon: {epsilon:4.2f}'\n",
    "            )\n",
    "        lengths.append(t_episode+1)\n",
    "        t_episode = 0\n",
    "        i_episode += 1\n",
    "    else:\n",
    "        t_episode += 1\n",
    "        \n",
    "    if len(replay_buffer) > replay_initial:\n",
    "        loss = compute_td_loss(batch_size)\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "#     if frame_idx % 10000 == 0:\n",
    "#         plot(frame_idx, all_rewards, losses)\n",
    "        \n",
    "    if frame_idx % 1000 == 0:\n",
    "        update_target(current_model, target_model)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot(frame_idx, returns, lengths, losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "a1 = np.array(lengths)\n",
    "b1 = np.array(returns)\n",
    "\n",
    "df = pd.DataFrame({\"lengths\" : a1, \"returns\" : b1})\n",
    "df.to_csv(\"breakout_eps1_dueling_dqn.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
